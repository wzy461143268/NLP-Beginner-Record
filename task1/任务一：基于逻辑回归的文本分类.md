# 任务一：基于逻辑回归的文本分类

要求使用Numpy实现文本分类，探究不同参数对训练效果的影响

## 载入数据

使用Pandas读入tsv数据

```python
test_data = pd.read_csv('data/test.tsv',sep='\t')
train_data = pd.read_csv('data/train.tsv',sep='\t')
```

## 按单词划分特征构建特征向量

```python
def BoW_tokenize(data):
    words_set=set()
    for sentence in data:
        #sentence=re.sub(r'[^\w\s]',' ',sentence) #去除标点符号
        words_set.update(sentence.split())
    return words_set #return a set of words

def BoW_vectorize(data,words_set):
    vector_size=len(words_set)
    feature_vector=np.zeros((len(data),vector_size),dtype=np.int8)
    vector_map=dict(zip(words_set,range(vector_size)))
    for sentence_index,sentence in enumerate(data):
        #sentence=re.sub(r'[^\w\s]',' ',sentence) #去除标点符号
        for word in sentence.split():
            if word in words_set:
                feature_vector[sentence_index,vector_map[word]]+=1
    # 使用n维向量表示句子，每个维度表示一个单词，值为单词出现的次数
    return feature_vector
```

在实际训练过程中出现了tokenize中提取的特征不包含vectorize过程中提取到的特征的情况。只能首先判断特征集是否包含特征；由于数量太大无法排除是否是程序问题暂时丢弃了溢出的特征。

## N-gram 特征提取

```python
def Ng_tokenize(data , n=2):
    words_set=set()
    for sentence in data:
        #sentence=re.sub(r'[^\w\s]',' ',sentence) # 去除标点符号
        word_list=sentence.split()
        for i in range(len(word_list)-n):
            words_set.add(' '.join(word_list[i:i+n]))
    
    return words_set

def Ng_vectorize(data,words_set,n=2):
    vector_size=len(words_set)
    feature_vector=np.zeros((len(data),vector_size),dtype=np.int8)
    vector_map=dict(zip(words_set,range(vector_size)))
    for sentence_index,sentence in enumerate(data):
        #sentence=re.sub(r'[^\w\s]',' ',sentence) # 去除标点符号
        word_list=sentence.split()
        for i in range(len(word_list)-n):
            if ' '.join(word_list[i:i+n]) in words_set:
                feature_vector[sentence_index,vector_map[' '.join(word_list[i:i+n])]]+=1
    return feature_vector
```

采用N-gram方法将n个单词拼接成小段落提取句子特征，实测将训练集特征提取后共有超过7万个特征，限于机器内存，没有实际进行测试

## 训练参数更新过程

根据P60的Softmax回归决策函数公式3.36：
$$
\begin{aligned}
\hat{\boldsymbol{y}} &=\operatorname{softmax}\left(\boldsymbol{W}^{\top} \boldsymbol{x}\right) \\
&=\frac{\exp \left(\boldsymbol{W}^{\top} \boldsymbol{x}\right)}{\mathbf{1}_{C}^{\top} \exp \left(\boldsymbol{W}^{\top} \boldsymbol{x}\right)}
\end{aligned}
$$
公式3.39给出的Softmax回归交叉熵损失函数：
$$
\begin{aligned}
\mathcal{R}(\boldsymbol{W}) &=-\frac{1}{N} \sum_{n=1}^{N} \sum_{c=1}^{C} \boldsymbol{y}_{c}^{(n)} \log \hat{\boldsymbol{y}}_{c}^{(n)} \\
&=-\frac{1}{N} \sum_{n=1}^{N}\left(\boldsymbol{y}^{(n)}\right)^{\mathrm{T}} \log \hat{\boldsymbol{y}}^{(n)}
\end{aligned}
$$
以及P61公式3.53给出的权重梯度下降更新方法：
$$
\boldsymbol{W}_{t+1} \leftarrow \boldsymbol{W}_{t}+\alpha\left(\frac{1}{N} \sum_{n=1}^{N} \boldsymbol{x}^{(n)}\left(\boldsymbol{y}^{(n)}-\hat{\boldsymbol{y}}_{W_{t}}^{(n)}\right)^{\top}\right)
$$
可以得到如下训练过程

```python
def train(train_X,train_y,val_X,val_y,batchsize=32,lr=1e0,epoch_number=100,class_number=5):
    iter_number=train_X.shape[0]//batchsize
    weight=np.zeros((train_X.shape[1],class_number))
    # 不同初始值的影响
    # weight=np.random.normal(0,1,[train_X.shape[1],class_number])
    train_loss_list=[]
    test_loss_list=[]
    for i in range(epoch_number):
        train_loss=0
        test_loss=0
        for j in tqdm(range(iter_number)):
            train_data=train_X[j*batchsize:j*batchsize+batchsize]
            y_train=train_y[j*batchsize:j*batchsize+batchsize]
            y=np.exp(train_data.dot(weight))   
            y_hat=np.divide(y.T,np.sum(y,axis=1)).T
            train_loss+= (-1/train_X.shape[0])*np.sum(np.multiply(y_train,np.log10(y_hat)))
            # 每个batch权重更新一次
            weight+=(lr/batchsize)*train_data.T.dot(y_train-y_hat)
       	    # 更新权重 
        y=np.exp(val_X.dot(weight))   
        y_hat=np.divide(y.T,np.sum(y,axis=1)).T
        test_loss= (-1/val_X.shape[0])*np.sum(np.multiply(val_y,np.log10(y_hat)))
        # print('train_loss:',train_loss," test_loss:",test_loss)
        train_loss_list.append(train_loss)
        test_loss_list.append(test_loss)
    
    return train_loss_list,test_loss_list,weight
```



## 测试结果

### val_rate=0.2,epoch=20, BoW法 ，batchsize=8，lr=1

![image-20230220233652506](F:\Git\NLP-Beginner-Record\task1\image\image-20230220233652506.png)

### val_rate=0.2,epoch=20, BoW法 ，batchsize=32,lr=1

![image-20230220233420642](F:\Git\NLP-Beginner-Record\task1\image\image-20230220233420642.png)

### val_rate=0.2,epoch=20, BoW法 ，batchsize=128,lr=1

![image-20230220233745714](F:\Git\NLP-Beginner-Record\task1\image\image-20230220233745714.png)

### val_rate=0.2,epoch=20, BoW法 ，batchsize=32,lr=10

![image-20230220233906569](F:\Git\NLP-Beginner-Record\task1\image\image-20230220233906569.png)

### val_rate=0.2,epoch=10, BoW法 ，batchsize=32,lr=1

![image-20230220233935285](C:\Users\wy461\AppData\Roaming\Typora\typora-user-images\image-20230220233935285.png)

### val_rate=0.2,epoch=40, BoW法 ，batchsize=128,lr=0.1

![image-20230220235235125](F:\Git\NLP-Beginner-Record\task1\image\image-20230220235235125.png)

### val_rate=0.2,epoch=40, BoW法 ，batchsize=128,lr=1

![image-20230221000223913](F:\Git\NLP-Beginner-Record\task1\image\image-20230221000223913.png)

### 测试总结

- 20个epoch下模型普遍训练不充分，测试集和训练集loss都在不断下降
- 学习率过大会严重影响模型的泛化性能，在学习率=10而其他参数不变的情况下，迅速出现了过拟合现象，过大的学习率可能使模型无法收敛
- batch size会影响模型泛化性能。其他参数不变batch size缩小时模型同样迅速过拟合，batch size设置过小可能会影响模型对数据特征的提取
- epoch数过小会导致模型训练不充分，在未过拟合、算力允许的情况下应该尽量将epoch数调大
- 适当加大学习率能加快模型收敛速度